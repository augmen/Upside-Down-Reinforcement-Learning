{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def random_policy(obs):\n",
    "    return np.random.randint(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "#Visualise agent function\n",
    "def visualise_agent(policy, command, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            current_command = deepcopy(command)\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            episode_return=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                network_input = torch.tensor(np.append(observation, current_command)).double()\n",
    "                policy_action = policy(network_input)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                episode_return+=reward\n",
    "                #time.sleep(0.1)\n",
    "                current_command[0]-= reward\n",
    "                current_command[1] = max(1, current_command[1]-1)\n",
    "                \n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps. Return = {}\".format(trial_i, t, episode_return))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise_agent(random_policy, command=[500, 500], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCNN_AGENT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(np.prod(env.observation_space.shape)+2, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, env.action_space.n)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "    \n",
    "    def create_optimizer(self, lr):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "class FCNN_AGENT_NEW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(np.prod(env.observation_space.shape)+2, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, env.action_space.n)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "    \n",
    "    def create_optimizer(self, lr):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def collect_experience(policy, replay_buffer, replay_size, last_few, n_episodes=100):\n",
    "    global i_episode\n",
    "    init_replay_buffer = deepcopy(replay_buffer)\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            command = sample_command(init_replay_buffer, last_few)\n",
    "            writer.add_scalar('Command desired reward', command[0], i_episode)    # write loss to a graph\n",
    "            writer.add_scalar('Command horizon', command[1], i_episode)    # write loss to a graph\n",
    "            observation = env.reset()\n",
    "            episode_mem = {'observation':[],\n",
    "                            'action':[],\n",
    "                            'reward':[]}\n",
    "            done=False\n",
    "            while not done:\n",
    "                network_input = torch.tensor(np.append(observation, command)).double()\n",
    "                action = policy(network_input)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                \n",
    "                episode_mem['observation'].append(observation)\n",
    "                episode_mem['action'].append(action)\n",
    "                episode_mem['reward'].append(reward)\n",
    "                \n",
    "                observation=new_observation\n",
    "                #command[0]-= reward\n",
    "                command[0] = max(1, command[0]-reward)\n",
    "                command[1] = max(1, command[1]-1)\n",
    "            episode_mem['return']=sum(episode_mem['reward'])\n",
    "            episode_mem['episode_len']=len(episode_mem['observation'])\n",
    "            replay_buffer.append(episode_mem)\n",
    "            i_episode+=1\n",
    "            if log_to_tensorboard: writer.add_scalar('Return', sum(episode_mem['reward']), i_episode)    # write loss to a graph\n",
    "            print(\"Episode {} finished after {} timesteps. Return = {}\".format(i_episode, len(episode_mem['observation']), sum(episode_mem['reward'])))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()\n",
    "    replay_buffer = sorted(replay_buffer, key=lambda x:x['return'])[-replay_size:]\n",
    "    return replay_buffer\n",
    "\n",
    "def sample_command(replay_buffer, last_few):\n",
    "    if len(replay_buffer)==0:\n",
    "        return [1, 1]\n",
    "    else:\n",
    "        command_samples = replay_buffer[-last_few:]\n",
    "        lengths = [mem['episode_len'] for mem in command_samples]\n",
    "        returns = [mem['return'] for mem in command_samples]\n",
    "        mean_return, std_return = np.mean(returns), np.std(returns)\n",
    "        command_horizon = np.mean(lengths)\n",
    "        desired_reward = np.random.uniform(mean_return, mean_return+std_return)\n",
    "        return [desired_reward, command_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(policy_net, replay_buffer, n_updates=100, batch_size=64):\n",
    "    all_costs = []\n",
    "    for i in range(n_updates):\n",
    "        batch_input = np.zeros((batch_size, np.prod(env.observation_space.shape)+2))\n",
    "        batch_label = np.zeros((batch_size))\n",
    "        for b in range(batch_size):\n",
    "            sample_episode = np.random.randint(0, len(replay_buffer))\n",
    "            sample_t1 = np.random.randint(0, len(replay_buffer[sample_episode]['observation']))\n",
    "            sample_t2 = len(replay_buffer[sample_episode]['observation'])\n",
    "            sample_horizon = sample_t2-sample_t1\n",
    "            sample_mem = replay_buffer[sample_episode]['observation'][sample_t1]\n",
    "            sample_desired_reward = sum(replay_buffer[sample_episode]['reward'][sample_t1:sample_t2])\n",
    "            network_input = np.append(sample_mem, [sample_desired_reward, sample_horizon])\n",
    "            label = replay_buffer[sample_episode]['action'][sample_t1]\n",
    "            batch_input[b] = network_input\n",
    "            batch_label[b] = label\n",
    "        batch_input = torch.tensor(batch_input).double()\n",
    "        batch_label = torch.tensor(batch_label).long()\n",
    "        pred = policy_net(batch_input)\n",
    "        cost = F.cross_entropy(pred, batch_label)\n",
    "        all_costs.append(cost.item())\n",
    "        cost.backward()\n",
    "        policy_net.optimizer.step()\n",
    "        policy_net.optimizer.zero_grad()\n",
    "    return np.mean(all_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_policy(policy_network):\n",
    "    def policy(obs):\n",
    "        action_prob = policy_network(obs)\n",
    "        action = np.argmax(action_prob.detach().numpy())\n",
    "        return action\n",
    "    return policy\n",
    "\n",
    "def create_stochastic_policy(policy_network):\n",
    "    def policy(obs):\n",
    "        action_prob = policy_network(obs)\n",
    "        action_sample = np.random.multinomial(1, action_prob.detach().numpy())\n",
    "        action = np.argmax(action_sample)\n",
    "        return action\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_episode=0\n",
    "replay_buffer = []\n",
    "replay_size = 600\n",
    "last_few = 50\n",
    "log_to_tensorboard = True \n",
    "\n",
    "batch_size = 32\n",
    "n_warm_up_episodes = 50\n",
    "n_episodes_per_iter = 50\n",
    "n_updates_per_iter = 200\n",
    "\n",
    "lr = 0.001\n",
    "agent = FCNN_AGENT().double()\n",
    "agent.create_optimizer(lr)\n",
    "\n",
    "stochastic_policy = create_stochastic_policy(agent)\n",
    "greedy_policy = create_greedy_policy(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP TRAINING VISUALISATION\n",
    "# SET UP TRAINING VISUALISATION\n",
    "if log_to_tensorboard: from torch.utils.tensorboard import SummaryWriter\n",
    "if log_to_tensorboard: writer = SummaryWriter() # we will use this to show our models performance on a graph using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect warm up episodes\n",
    "replay_buffer = collect_experience(random_policy, replay_buffer, replay_size, last_few, n_warm_up_episodes, log_to_tensorboard)\n",
    "train_net(agent, replay_buffer, n_updates=n_updates_per_iter, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.load_state_dict(torch.load('checkpoints/lunar_lander_32x32_checkpoint_0.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 10000\n",
    "for i in range(n_iters):\n",
    "    replay_buffer = collect_experience(stochastic_policy, replay_buffer, replay_size, last_few, n_episodes_per_iter, log_to_tensorboard)\n",
    "    train_net(agent, replay_buffer, n_updates=n_updates_per_iter, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 84 timesteps. Return = -134.1428887684655\n",
      "Episode 1 finished after 62 timesteps. Return = -143.282848576283\n",
      "Episode 2 finished after 72 timesteps. Return = -0.15610989654310004\n",
      "Episode 3 finished after 106 timesteps. Return = -72.03684838919166\n",
      "Episode 4 finished after 105 timesteps. Return = -37.44217835732849\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, command=[250, 200], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 97 timesteps. Return = -89.47575725612703\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(stochastic_policy, command=[250, 200], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(agent.state_dict(), 'checkpoints/lunar_lander_32x32_checkpoint_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([mem['return'] for mem in replay_buffer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(policy_net, replay_buffer, n_updates=100, batch_size=64):\n",
    "    all_costs = []\n",
    "    for i in range(n_updates):\n",
    "        batch_input = np.zeros((batch_size, np.prod(env.observation_space.shape)+2))\n",
    "        batch_label = np.zeros((batch_size))\n",
    "        for b in range(batch_size):\n",
    "            sample_episode = np.random.randint(0, len(replay_buffer))\n",
    "            sample_horizon = np.random.randint(1, len(replay_buffer[sample_episode]['observation'])+1)\n",
    "            sample_mem_idx = np.random.randint(0, len(replay_buffer[sample_episode]['observation'])+1-sample_horizon)\n",
    "            sample_mem = replay_buffer[sample_episode]['observation'][sample_mem_idx]\n",
    "            sample_desired_reward = sum(replay_buffer[sample_episode]['reward'][sample_mem_idx:sample_mem_idx+sample_horizon])\n",
    "            network_input = np.append(sample_mem, [sample_desired_reward, sample_horizon])\n",
    "            label = replay_buffer[sample_episode]['action'][sample_mem_idx]\n",
    "            batch_input[b] = network_input\n",
    "            batch_label[b] = label\n",
    "        batch_input = torch.tensor(batch_input).double()\n",
    "        batch_label = torch.tensor(batch_label).long()\n",
    "        pred = policy_net(batch_input)\n",
    "        cost = F.cross_entropy(pred, batch_label)\n",
    "        all_costs.append(cost.item())\n",
    "        cost.backward()\n",
    "        policy_net.optimizer.step()\n",
    "        policy_net.optimizer.zero_grad()\n",
    "    return np.mean(all_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(policy_net, episode_mem, n_samples = 5): #stochastic gradient descent\n",
    "    all_costs = []\n",
    "    for i in range(n_samples):\n",
    "        sample_horizon = np.random.randint(1, len(episode_mem['observation'])+1)\n",
    "        sample_mem_idx = np.random.randint(0, len(episode_mem['observation'])+1-sample_horizon)\n",
    "        sample_mem = episode_mem['observation'][sample_mem_idx]\n",
    "        sample_desired_reward = sum(episode_mem['reward'][sample_mem_idx:sample_mem_idx+sample_horizon])\n",
    "        network_input = torch.tensor(np.append(sample_mem, [sample_desired_reward, sample_horizon])).double()\n",
    "        label = torch.tensor([episode_mem['action'][sample_mem_idx]]).double()\n",
    "        \n",
    "        pred = policy_net(network_input)\n",
    "        cost = F.binary_cross_entropy(pred, label)\n",
    "        all_costs.append(cost.item())\n",
    "        cost.backward()\n",
    "        policy_net.optimizer.step()\n",
    "        policy_net.optimizer.zero_grad()\n",
    "    return np.mean(all_costs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy_net, n_episodes=100):\n",
    "    global i_episode\n",
    "    global epsilon\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = {'observation':[],\n",
    "                            'action':[],\n",
    "                            'reward':[],\n",
    "                            'done':[]}\n",
    "            done=False\n",
    "            while not done:\n",
    "                network_input = torch.tensor(np.append(observation, [desired_reward, command_horizon])).double()\n",
    "                action_prob = policy_net(network_input)\n",
    "                action = np.random.binomial(1, action_prob.item())\n",
    "                #action = int(action_prob.item()>0.5)\n",
    "                if np.random.rand()<epsilon: action = np.random.randint(0, 2)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                \n",
    "                episode_mem['observation'].append(observation)\n",
    "                episode_mem['action'].append(action)\n",
    "                episode_mem['reward'].append(reward)\n",
    "                episode_mem['done'].append(done)\n",
    "                \n",
    "                observation=new_observation\n",
    "                epsilon*=0.999\n",
    "            episode_mem['return']=sum(episode_mem['reward'])\n",
    "            episode_mem['episode_len']=len(episode_mem['observation'])\n",
    "            mean_cost = train_net(policy_net, episode_mem)\n",
    "            \n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Epsilon={} Mean Cost={}\".format(i_episode, len(episode_mem['observation']), epsilon, mean_cost))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
